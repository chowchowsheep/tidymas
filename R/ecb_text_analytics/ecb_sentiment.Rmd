---
title: "ECB Text Analytics"
output: html_notebook
---

```{r, message = FALSE}
library(dplyr)
library(tidytext)
library(lubridate)
library(ggplot2)
library(tidyverse)
```

# Read in ECB speeches as scraped

```{r}
df <- read.csv("data/ecb_speeches.csv", stringsAsFactors = FALSE) %>%
  mutate(date = ymd(date), type = as.factor(type), speaker = as.factor(speaker))

# Extract press speech and answers
df_ecb <- df %>%
  #mutate(text = iconv(text, to="latin1", sub="")) %>%
  filter(type %in% c("press speech", "answer")) %>%
  group_by(date) %>%
  summarise(text = paste0(text, collapse = "."))

str(df_ecb)
```

# Preprocess by tokenising words and removing stop words

```{r}
library(tidytext)
source("text_analytics.R")

custom_stop_words = bind_rows(data_frame(word = c(as.character(1980:2030)),
                                          lexicon = c("custom")),
                               stop_words)

ecb_unigrams <- df_ecb %>%
  make_ngrams(custom_stop_words = custom_stop_words) 

ecb_unigrams %>%
  count_ngrams %>%
  filter(n > 1000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()
```

# Single word sentiment analysis

## List of top words and their associated sentiments

```{r}
# Compare how words categorization differs across sets
ecb_unigrams %>%
  count_ngrams %>%
  add_sentiments(c("afinn", "loughran", "nrc", "bing")) %>%
  compare_words_sentiment

```

## Calculating a sentiment score

```{r}
ecb_sentiment <- ecb_unigrams %>%
  add_sentiments %>%
  filter(sentiment %in% c("positive", "negative")) %>%
  group_by(method, date, sentiment) %>%
  summarise(n = sum(n)) %>%
  ungroup %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative,
         negative = -negative)

afinn <- ecb_unigrams %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(date) %>%
  summarise(sentiment = sum(score * n)) %>%
  mutate(method = "AFINN")

all_sentiment <- bind_rows(afinn, ecb_sentiment) 

g <- all_sentiment %>%  
  gather(type, value, -date, -method) %>%
  ggplot(aes(date, value, colour = type)) 
g + geom_line() + 
  geom_hline(yintercept = 0) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

# High scoring words from the dictionary

```{r}
ecb_unigrams %>%
  add_sentiments %>%
  filter(sentiment %in% c("positive", "negative")) %>%
  group_by(method, word, sentiment) %>%
  summarise(score = sum(n * ifelse(sentiment == "negative", -1, 1))) %>%
  group_by(method) %>%
  top_n(10, abs(score)) %>%
  ungroup %>%
  mutate(word = reorder(word, score)) %>%
  ggplot(aes(word, score, fill=sentiment)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~method, scales = "free_y") +
    coord_flip()
```

# Calculate ranking for inverse document frequency

```{r}
unigrams_wt <- ecb_unigrams %>%
  group_by(date) %>%
  count_ngrams %>%
  bind_tf_idf(word, date, n) %>%
  select(-tf, -idf) %>%
  arrange(desc(tf_idf)) %>%
  ungroup

unigrams_wt %>% 
  top_n(20) %>%
  ggplot(aes(factor(word, levels = rev(unique(word))), tf_idf, fill = as.factor(year(date)))) +
    geom_col() +
    labs(x = NULL, y = "tf-idf") +
    coord_flip()
```

# Words with high weights

Below we see words with high weights due to the uniqueness of their appearance for each of the years

```{r}
unigrams_wt %>% 
  ungroup %>%
  filter(year(date) > 2008) %>%
  group_by(year = as.factor(year(date))) %>% 
  top_n(15, tf_idf) %>% 
  ungroup %>%
  ggplot(aes(factor(word, levels = rev(unique(word))), tf_idf, fill = year)) +
    geom_col(show.legend = FALSE) +
    labs(x = NULL, y = "tf-idf") +
    facet_wrap(~year, ncol = 3, scales = "free") +
    coord_flip()
```

# Calculation of weighted scores

```{r}
weighted_score <- unigrams_wt %>%
  add_sentiments %>%
  filter(sentiment %in% c("positive", "negative")) %>%
  mutate(score = tf_idf * ifelse(sentiment == "positive", 1, -1)) %>%
  group_by(date, method) %>%
  summarise(score = sum(score))

weighted_afinn <- unigrams_wt %>%
  inner_join(get_sentiments("afinn")) %>%
  mutate(score = score * tf_idf) %>%
  group_by(date) %>%
  summarise(score = sum(score)) %>%
  mutate(method = "afinn")

weighted_score <- bind_rows(weighted_afinn, weighted_score) 

# Centre all scores and plot on the same chart
weighted_score %>%
  group_by(method) %>%
  mutate(score = scale(score)) %>%
  ggplot(aes(date, score, colour = method)) +
  geom_line()

# Plot all scores on different charts
ggplot(weighted_score, aes(date, score, colour = method)) + 
  geom_line() + 
  geom_hline(yintercept = 0) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

## TODO 1: Comparison of weighted vs unweighted on the same chart
## TODO 2: Words that produced the highest contributions in the recent years
## TODO 3: Subtracting negation words using bigrams

## Diagnostics of using different dictionaries

### Comparison of weighted scores across the different dictionaries

Weighted scores look more or less correlated, although not very strongly

```{r}
library(GGally)
weighted_score %>%
  spread(method, score) %>%
  select(-date) %>%
  ggpairs
```


# Dealing with bigrams and negation words

```{r}

bigrams_separated <- df_ecb %>%
  make_ngrams(2, remove_stop_words = FALSE)

bigram_date_counts <- bigrams_separated %>% 
  group_by(date) %>%
  count_ngrams() %>%
  ungroup

# Trigrams
df_ecb %>% make_ngrams(3) %>%
  count_ngrams
```

```{r}
bigrams_united <- df_ecb %>%
  make_ngrams(2, remove_stop_words = FALSE) %>%
  unite(bigram, word1, word2, sep = " ")

bigram_tf_idf <- bigrams_united %>%
  count(year = year(date), bigram) %>%
  bind_tf_idf(bigram, year, n) %>%
  arrange(desc(tf_idf))

bigrams_filtered %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)

AFINN <- get_sentiments("afinn")

negation_words <- c("not", "no", "never", "without", "negative", "weak")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()

negated_words %>%
  mutate(contribution = n * score) %>%
  group_by(word1) %>%
  top_n(10, desc(abs(contribution))) %>%
  arrange(contribution) %>%
  ungroup %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~word1, ncol = 2, scales = "free_y") +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()
```

# Visualizing network of bigrams

```{r}
df_ecb %>%
  make_ngrams(2) %>%
  count_ngrams() %>%
  visualize_bigrams(min.n = 100)
```

# Counting and correlation among sections

```{r}
## POSSIBLE IDEA: Break into individual years, and find change in correlation between words for key words like inflation,
##                rate hikes, growth etc?

ecb_year_words <- ecb_unigrams %>%
  mutate(section = year(date))

library(widyr)
word_cors <- ecb_year_words %>%
  filter(section == 2016) %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, date, sort = TRUE)
 
word_cors %>%
  filter(item1 %in% c("inflation", "growth")) %>%
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()
```

# Unrelated stock mining

```{r}
library(tm.plugin.webmining)
library(purrr)
library(tidyverse)
library(tidytext)

company <- c("Microsoft", "Apple")
symbol <- c("MSFT", "AAPL")

download_articles <- function(symbol) {
  WebCorpus(GoogleFinanceSource(paste0("NASDAQ:", symbol)))
}

stock_articles <- data_frame(company = company,
                             symbol = symbol) %>%
  mutate(corpus = map(symbol, download_articles))

stock_tokens <- stock_articles %>%
  unnest(map(corpus, tidy)) %>%
  unnest_tokens(word, text) %>%
  select(company, datetimestamp, word, id, heading)

library(stringr)

stock_tf_idf <- stock_tokens %>%
  count(company, word) %>%
  filter(!str_detect(word, "\\d+")) %>%
  bind_tf_idf(word, company, n) %>%
  arrange(-tf_idf)

```

```{r}
stock_tokens %>%
  count(word) %>%
  inner_join(get_sentiments("loughran"), by = "word") %>%
  group_by(sentiment) %>%
  top_n(5, n) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ sentiment, scales = "free") +
  ylab("Frequency of this word in the recent financial articles")
```


```{r}
stock_sentiment_count <- stock_tokens %>%
  inner_join(get_sentiments("loughran"), by = "word") %>%
  count(sentiment, company) %>%
  spread(sentiment, n, fill = 0)

stock_sentiment_count %>%
  mutate(score = (positive - negative) / (positive + negative)) %>%
  mutate(company = reorder(company, score)) %>%
  ggplot(aes(company, score, fill = score > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = "Company",
       y = "Positivity score among 20 recent news articles")

```