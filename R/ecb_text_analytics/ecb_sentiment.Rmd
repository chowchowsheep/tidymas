---
title: "ECB Text Analytics"
output: html_notebook
---

```{r, message = FALSE}
library(dplyr)
library(tidytext)
library(lubridate)
library(ggplot2)
library(tidyverse)
```

# Read in ECB speeches as scraped

```{r}
df <- read.csv("data/ecb_speeches.csv", stringsAsFactors = FALSE) %>%
  mutate(date = ymd(date), type = as.factor(type), speaker = as.factor(speaker))

# Extract press speech and answers
df_ecb <- df %>%
  #mutate(text = iconv(text, to="latin1", sub="")) %>%
  filter(type %in% c("press speech", "answer")) %>%
  group_by(date) %>%
  summarise(text = paste0(text, collapse = "."))

str(df_ecb)
```

# Preprocess by tokenising words and removing stop words

```{r}
library(tidytext)
source("text_analytics.R")

custom_stop_words = bind_rows(data_frame(word = c(as.character(1980:2030)),
                                          lexicon = c("custom")),
                               stop_words)

ecb_unigrams <- df_ecb %>%
  make_ngrams(custom_stop_words = custom_stop_words) 

ecb_unigrams %>%
  count_ngrams %>%
  filter(n > 1000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()
```

# Single word sentiment analysis

## List of top words and their associated sentiments

```{r}
# Compare how words categorization differs across sets
ecb_unigrams %>%
  count_ngrams %>%
  add_sentiments(c("afinn", "loughran", "nrc", "bing")) %>%
  compare_words_sentiment

```

## Calculating a sentiment score

```{r}
ecb_scores <- ecb_unigrams %>%
  add_sentiments(all = TRUE) %>%
  group_by(date) %>%
  calc_sentiment_score(wt = "n")

g <- ecb_scores %>%  
  ggplot(aes(date, score, colour = method)) 
g + geom_line() + 
  geom_hline(yintercept = 0) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

# High scoring words from the dictionary

```{r}
ecb_unigrams %>%
  add_sentiments(all = TRUE) %>%
  group_by(word) %>%
  calc_sentiment_score(wt = "n") %>%
  group_by(method) %>%
  top_n(10, abs(score)) %>%
  ungroup %>%
  mutate(word = reorder(word, score)) %>% 
  ggplot(aes(word, score, fill=factor(score > 0))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~method, scales = "free_y") +
    coord_flip()
```

# Calculate ranking for inverse document frequency

```{r}
unigrams_wt <- ecb_unigrams %>%
  group_by(date) %>%
  count_ngrams %>%
  bind_tf_idf(word, date, n) %>%
  ungroup

unigrams_wt %>% 
  top_n(20) %>%
  ggplot(aes(factor(word, levels = rev(unique(word))), tf_idf, fill = as.factor(year(date)))) +
    geom_col() +
    labs(x = NULL, y = "tf-idf") +
    coord_flip()
```

# Words with high weights

Below we see words with high weights due to the uniqueness of their appearance for each of the years

```{r}
unigrams_wt %>% 
  ungroup %>%
  filter(year(date) > 2008) %>%
  group_by(year = as.factor(year(date))) %>% 
  top_n(15, tf_idf) %>% 
  ungroup %>%
  ggplot(aes(factor(word, levels = rev(unique(word))), tf_idf, fill = year)) +
    geom_col(show.legend = FALSE) +
    labs(x = NULL, y = "tf-idf") +
    facet_wrap(~year, ncol = 3, scales = "free") +
    coord_flip()
```

# Calculation of weighted scores

Adjusting for weighted score, we compare with the unweighted scores. We see that directionally, both looks quite similar. 

```{r, fig.height = 5}
weighted_scores <- unigrams_wt %>%
  add_sentiments(all = TRUE) %>%
  group_by(date, method) %>%
  calc_sentiment_score(wt = "tf_idf") 

combined_scores <- bind_rows(ecb_scores %>% mutate(weight = "unweighted"),
                            weighted_scores %>% mutate(weight = "weighted"))

# Centre all scores and plot weight and unweighted on the same chart
combined_scores %>%
  group_by(method, weight) %>%
  mutate(score = scale(score)) %>%
  ggplot(aes(date, score, colour = weight)) +
  geom_line() +
  facet_wrap(~method, ncol = 1, scales = "free_y") +
  geom_hline(yintercept = 0, lwd=0.3, lty = 5) +
  ggtitle("Standardized sentiment scores")
```

# Top weighting words

We analyse the in the weighted sentiment, which are the words which are the top contributors to the sentiment scores in each of the recent years since 2008

```{r fig.height = 13, fig.width=10}
unigrams_wt %>%
  add_sentiments(all = TRUE) %>%
  group_by(date, word) %>%
  calc_sentiment_score(wt = "tf_idf") %>%
  group_by(year = year(date), word, method) %>%
  summarise(score = sum(score)) %>%
  group_by(year, method) %>%
  filter(year > 2012) %>%
  top_n(10, abs(score)) %>%
  ungroup %>%
  ggplot(aes(factor(word, levels = unique(word[order(score)])), score, fill = factor(sign(score)))) +
    geom_col(show.legend = FALSE) +
    labs(x = NULL, y = "tf-idf") +
    facet_grid(year~method, scales = "free") +
    coord_flip()
```

## TODO 3: Subtracting negation words using bigrams

## Diagnostics of using different dictionaries

### Comparison of weighted scores across the different dictionaries

Weighted scores look more or less correlated, although not very strongly

```{r}
library(GGally)
weighted_scores %>%
  select(-negative, -positive) %>%
  spread(method, score) %>%
  select(-date) %>%
  ggpairs
```


# Dealing with bigrams and negation words

```{r, fig.height=10, fig.width=8}
bigrams_separated <- df_ecb %>%
  make_ngrams(2, remove_stop_words = FALSE)

#bigrams_united <- bigrams_separated %>%
#  unite(bigram, word1, word2, sep = " ")

#bigram_tf_idf <- bigrams_united %>%
#  count(year = year(date), bigram) %>%
#  bind_tf_idf(bigram, year, n) %>%
#  arrange(desc(tf_idf))

negation_words <- c("not", "no", "never", "without", "negative", "weak")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  #inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  ungroup()

# adjust unweighted
#negated_words %>% 

# adjust weighted
negative_scores <- negated_words %>% 
  rename(n2 = n, word = word2) %>%
  inner_join(unigrams_wt, by = c("date", "word")) %>%
  mutate(tf_idf2 = (n2 * 2 * -1)/n * tf_idf) %>%
  add_sentiments(all = TRUE) %>%
  group_by(date) %>%
  calc_sentiment_score(wt = "tf_idf2") 



temp <- negative_scores %>%
  select(-negative, -positive) %>%
  rename(score2 = score) %>%
  right_join(weighted_scores, by = c("date", "method")) %>%
  mutate(score3 = ifelse(is.na(score2), score, score + score2))

temp %>% select(date, method, score, score2, score3) %>%
  gather(score_type, scores, -date, -method) %>%
  ggplot(aes(date, scores, colour = score_type)) +
  geom_line() +
  facet_wrap(~method, ncol=1, scales = "free_y") +
  geom_hline(yintercept = 0, lwd=0.3, lty = 5) 

# weighted_scores
# weighted_scores %>% inner_join(negative_scores, by = c("date", "method"))
# 
# negated_words %>%
#   rename(word = word2) %>%
#   add_sentiments(all = TRUE) %>%
#   group_by(date, word1, word) %>%
#   calc_sentiment_score(wt = "") %>%
#   mutate(contribution = n * score) %>%
#   group_by(word1) %>%
#   top_n(10, desc(abs(contribution))) %>%
#   arrange(contribution) %>%
#   ungroup %>%
#   mutate(word2 = reorder(word2, contribution)) %>%
#   ggplot(aes(word2, n * score, fill = n * score > 0)) +
#   geom_col(show.legend = FALSE) +
#   facet_wrap(~word1, ncol = 2, scales = "free_y") +
#   xlab("Words preceded by \"not\"") +
#   ylab("Sentiment score * number of occurrences") +
#   coord_flip() +
#   scale_color_manual(labels = c("original", "negatives", "adjusted"), values = c("red", "green", "blue"))

```

# Visualizing network of bigrams

```{r}
df_ecb %>%
  make_ngrams(2) %>%
  count_ngrams() %>%
  visualize_bigrams(min.n = 100)
```

# Counting and correlation among sections

```{r}
## POSSIBLE IDEA: Break into individual years, and find change in correlation between words for key words like inflation,
##                rate hikes, growth etc?

ecb_year_words <- ecb_unigrams %>%
  mutate(section = year(date))

library(widyr)
word_cors <- ecb_year_words %>%
  filter(section == 2016) %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, date, sort = TRUE)
 
word_cors %>%
  filter(item1 %in% c("inflation", "growth")) %>%
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()
```

# Unrelated stock mining

```{r}
library(tm.plugin.webmining)
library(purrr)
library(tidyverse)
library(tidytext)

company <- c("Microsoft", "Apple")
symbol <- c("MSFT", "AAPL")

download_articles <- function(symbol) {
  WebCorpus(GoogleFinanceSource(paste0("NASDAQ:", symbol)))
}

stock_articles <- data_frame(company = company,
                             symbol = symbol) %>%
  mutate(corpus = map(symbol, download_articles))

stock_tokens <- stock_articles %>%
  unnest(map(corpus, tidy)) %>%
  unnest_tokens(word, text) %>%
  select(company, datetimestamp, word, id, heading)

library(stringr)

stock_tf_idf <- stock_tokens %>%
  count(company, word) %>%
  filter(!str_detect(word, "\\d+")) %>%
  bind_tf_idf(word, company, n) %>%
  arrange(-tf_idf)

```

```{r}
stock_tokens %>%
  count(word) %>%
  inner_join(get_sentiments("loughran"), by = "word") %>%
  group_by(sentiment) %>%
  top_n(5, n) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ sentiment, scales = "free") +
  ylab("Frequency of this word in the recent financial articles")
```


```{r}
stock_sentiment_count <- stock_tokens %>%
  inner_join(get_sentiments("loughran"), by = "word") %>%
  count(sentiment, company) %>%
  spread(sentiment, n, fill = 0)

stock_sentiment_count %>%
  mutate(score = (positive - negative) / (positive + negative)) %>%
  mutate(company = reorder(company, score)) %>%
  ggplot(aes(company, score, fill = score > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = "Company",
       y = "Positivity score among 20 recent news articles")

```