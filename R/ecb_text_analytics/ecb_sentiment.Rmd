---
title: "ECB Text Analytics"
output: html_notebook
---

```{r, message = FALSE}
library(dplyr)
library(tidytext)
library(lubridate)
library(ggplot2)
library(tidyverse)
```

# Read in ECB speeches as scraped

```{r}
df <- read.csv("data/ecb_speeches.csv", stringsAsFactors = FALSE) %>%
  mutate(date = ymd(date), type = as.factor(type), speaker = as.factor(speaker))

# Extract press speech and answers
df_ecb <- df %>%
  mutate(text = iconv(text, to="latin1", sub="")) %>%
  filter(type %in% c("press speech", "answer")) %>%
  group_by(date) %>%
  summarise(text = paste0(text, collapse = "."))

str(df_ecb)
```

# Preprocess by tokenising words and removing stop words

```{r}
library(tidytext)
source("text_analytics.R")

custom_stop_words = bind_rows(data_frame(word = c(as.character(1980:2030)),
                                          lexicon = c("custom")),
                               stop_words)

ecb_unigrams <- df_ecb %>%
  make_ngrams(custom_stop_words = custom_stop_words)

ecb_unigrams %>%
  count_ngrams %>%
  filter(n > 1000) %>%
  #mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()
```

# Single word sentiment analysis

```{r}
afinn <- ecb_unigrams %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(date) %>%
  summarise(sentiment = sum(score)) %>%
  mutate(method = "AFINN")

bing_nrc_lou <- bind_rows(ecb_unigrams %>%
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing"),
                          ecb_unigrams %>%
                            inner_join(get_sentiments("nrc") %>%
                                         filter(sentiment %in% c("positive", "negative"))) %>%
                            mutate(method = "NRC"),
                          ecb_unigrams %>%
                            inner_join(get_sentiments("loughran") %>%
                                         filter(sentiment %in% c("positive", "negative"))) %>%
                            mutate(method = "Loughran")) %>%
  count(method, date = date, sentiment)%>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

all_sentiment <- bind_rows(afinn, bing_nrc_lou) %>%
  group_by(method) %>%
  mutate(negative = -negative, 
         positive = positive,
         sentiment = sentiment)

g <- all_sentiment %>%  
  gather(type, value, -date, -method) %>%
  ggplot(aes(date, value, colour = type)) 
g + geom_line() + 
  geom_hline(yintercept = 0) +
  facet_wrap(~method, ncol = 1, scales = "free_y")

```

```{r}
sentiment_words <- bind_rows(ecb_unigrams %>%
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing"),
                          ecb_unigrams %>%
                            inner_join(get_sentiments("nrc") %>%
                                         filter(sentiment %in% c("positive", "negative"))) %>%
                            mutate(method = "NRC"),
                          ecb_unigrams %>%
                            inner_join(get_sentiments("loughran") %>%
                                         filter(sentiment %in% c("positive", "negative"))) %>%
                            mutate(method = "Loughran")) %>%
  count(method, word, sentiment)

sentiment_words %>%
  mutate(score = n * ifelse(sentiment == "negative", -1, 1)) %>%
  group_by(method) %>%
  top_n(10, abs(score)) %>%
  ungroup %>%
  mutate(word = reorder(word, score)) %>%
  ggplot(aes(word, score, fill=sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, scales = "free_y") +
  coord_flip()


```

```{r}
# bing_word_counts <- ecb_unigrams %>%
#   inner_join(get_sentiments("bing")) %>%
#   group_by(year = year(date)) %>%
#   count(word, sentiment, sort = TRUE) %>%
#   ungroup()
# 
# bing_word_counts %>%
#   group_by(sentiment, year) %>%
#   top_n(5) %>%
#   ungroup() %>%
#   mutate(word = reorder(word, n)) %>%
#   ggplot(aes(word, n, fill = sentiment)) +
#   geom_col() +
#   facet_wrap(~year, scales = "free_y") +
#   labs(y = "Contribution to sentiment",
#        x = NULL) +
#   coord_flip()

bing_word_counts <- ecb_unigrams %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```
```{r}
ecb_words <- ecb_unigrams %>%
  group_by(date) %>%
  count_ngrams %>%
  ungroup

total_words <- ecb_words %>%
  group_by(date) %>%
  summarize(total = sum(n))

meeting_words <- left_join(ecb_words, total_words)

g <- ggplot(meeting_words, aes(n/total), fill = year(date))
g + geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.005) +
  facet_wrap(~year(date), ncol = 3, scales = "free_y")

```
# Ranking of the words

According to Zipf's law, frequency of a word is inversely proportional to its rank

```{r}
freq_by_rank <- meeting_words %>%
  group_by(year = year(date)) %>%
  mutate(rank = row_number(),
         'term frequency' = n/total)

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = as.factor(year))) + 
  geom_line(size = 1.2, alpha = 0.8) + 
  scale_x_log10() +
  scale_y_log10()
```

# Calculate ranking for inverse document frequency

```{r}
meeting_words <- meeting_words %>%
  bind_tf_idf(word, date, n)

meeting_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))

plot_ecb <- meeting_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))

plot_ecb %>% 
  top_n(20) %>%
  ggplot(aes(word, tf_idf, fill = as.factor(year(date)))) +
  geom_col() +
  labs(x = NULL, y = "tf-idf") +
  coord_flip()


```

# Words with high weights

Below we see words with high weights due to the uniqueness of their appearance for each of the years

```{r}
plot_ecb %>% 
  ungroup %>%
  filter(year(date) > 2008) %>%
  group_by(year = as.factor(year(date))) %>% 
  top_n(15, tf_idf) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = year)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~year, ncol = 3, scales = "free") +
  coord_flip()
```
```{r}

bigrams_separated <- df_ecb %>%
  make_ngrams(2, remove_stop_words = FALSE)

bigram_date_counts <- bigrams_separated %>% 
  group_by(date) %>%
  count_ngrams() %>%
  ungroup

# Trigrams
df_ecb %>% make_ngrams(3) %>%
  count_ngrams
```

```{r}
bigrams_united <- df_ecb %>%
  make_ngrams(2, remove_stop_words = FALSE) %>%
  unite(bigram, word1, word2, sep = " ")

bigram_tf_idf <- bigrams_united %>%
  count(year = year(date), bigram) %>%
  bind_tf_idf(bigram, year, n) %>%
  arrange(desc(tf_idf))

bigrams_filtered %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)

AFINN <- get_sentiments("afinn")

negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()

negated_words %>%
  mutate(contribution = n * score) %>%
  group_by(word1) %>%
  top_n(10, desc(abs(contribution))) %>%
  arrange(contribution) %>%
  ungroup %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~word1, ncol = 2, scales = "free_y") +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()
```

# Visualizing network of bigrams

```{r}
df_ecb %>%
  make_ngrams(2) %>%
  count_ngrams() %>%
  visualize_bigrams(min.n = 100)
```

# Counting and correlation among sections

```{r}
## POSSIBLE IDEA: Break into individual years, and find change in correlation between words for key words like inflation,
##                rate hikes, growth etc?

ecb_year_words <- ecb_unigrams %>%
  mutate(section = year(date))

library(widyr)
word_cors <- ecb_year_words %>%
  filter(section == 2016) %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, date, sort = TRUE)
 
word_cors %>%
  filter(item1 %in% c("inflation", "growth")) %>%
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()
```

```{r}
library(tm.plugin.webmining)
library(purrr)
library(tidyverse)
library(tidytext)

company <- c("Microsoft", "Apple")
symbol <- c("MSFT", "AAPL")

download_articles <- function(symbol) {
  WebCorpus(GoogleFinanceSource(paste0("NASDAQ:", symbol)))
}

stock_articles <- data_frame(company = company,
                             symbol = symbol) %>%
  mutate(corpus = map(symbol, download_articles))

stock_tokens <- stock_articles %>%
  unnest(map(corpus, tidy)) %>%
  unnest_tokens(word, text) %>%
  select(company, datetimestamp, word, id, heading)

library(stringr)

stock_tf_idf <- stock_tokens %>%
  count(company, word) %>%
  filter(!str_detect(word, "\\d+")) %>%
  bind_tf_idf(word, company, n) %>%
  arrange(-tf_idf)

```

```{r}
stock_tokens %>%
  count(word) %>%
  inner_join(get_sentiments("loughran"), by = "word") %>%
  group_by(sentiment) %>%
  top_n(5, n) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ sentiment, scales = "free") +
  ylab("Frequency of this word in the recent financial articles")
```


```{r}
stock_sentiment_count <- stock_tokens %>%
  inner_join(get_sentiments("loughran"), by = "word") %>%
  count(sentiment, company) %>%
  spread(sentiment, n, fill = 0)

stock_sentiment_count %>%
  mutate(score = (positive - negative) / (positive + negative)) %>%
  mutate(company = reorder(company, score)) %>%
  ggplot(aes(company, score, fill = score > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = "Company",
       y = "Positivity score among 20 recent news articles")

```