---
title: "ECB Text Analytics"
output:
<<<<<<< HEAD
  html_document: default
  html_notebook: default
=======
  html_document:
    df_print: paged
>>>>>>> cb63b95588d5bb45ea768e3cd25a3e360b80a0d8
---

```{r setup, message = FALSE}
library(dplyr)
library(tidytext)
library(lubridate)
library(ggplot2)
library(tidyverse)
<<<<<<< HEAD
=======
library(tidytext)
>>>>>>> cb63b95588d5bb45ea768e3cd25a3e360b80a0d8
source("text_analytics.R")
```

# Read in ECB speeches as scraped

```{r read_ecb_speeches}
df <- read.csv("data/ecb_speeches.csv", stringsAsFactors = FALSE) %>%
  mutate(date = ymd(date), type = as.factor(type), speaker = as.factor(speaker))

# Extract press speech and answers
df_ecb <- df %>%
  #mutate(text = iconv(text, to="latin1", sub="")) %>%
  filter(type %in% c("speech", "answer")) %>%
  group_by(date) %>%
  summarise(text = paste0(text, collapse = "."))

str(df_ecb)
```

# Preprocess by tokenising words and removing stop words

<<<<<<< HEAD
```{r}
=======
```{r text_preprocessing}
>>>>>>> cb63b95588d5bb45ea768e3cd25a3e360b80a0d8
#Remove years as stop words
custom_stop_words = bind_rows(data_frame(word = c(as.character(1980:2030)),
                                          lexicon = c("custom")),
                               stop_words)

ecb_unigrams <- df_ecb %>%
  make_ngrams(custom_stop_words = custom_stop_words) 

ecb_unigrams %>%
  count_ngrams %>%
  filter(n > 1000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()
```

# Single word sentiment analysis

## List of top words and their associated sentiments

```{r 1gram_lexicon_comparison}
# Compare how words categorization differs across sets
ecb_unigrams %>%
  count_ngrams %>%
  add_sentiments(c("afinn", "loughran", "nrc", "bing")) %>%
  compare_words_sentiment

```

## Calculating a sentiment score

```{r 1gram_sentiment_scoring}
ecb_scores <- ecb_unigrams %>%
  add_sentiments(all = TRUE) %>%
  group_by(date) %>%
  calc_sentiment_score(wt = "n")

g <- ecb_scores %>%  
  ggplot(aes(date, score, colour = method)) 
g + geom_line() + 
  geom_hline(yintercept = 0) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

# High scoring words from the dictionary

```{r 1gram_top_abs_scorers}
ecb_unigrams %>%
  add_sentiments(all = TRUE) %>%
  group_by(word) %>%
  calc_sentiment_score(wt = "n") %>%
  group_by(method) %>%
  top_n(10, abs(score)) %>%
  ungroup %>%
  mutate(word = reorder(word, score)) %>% 
  ggplot(aes(word, score, fill=factor(score > 0))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~method, scales = "free_y") +
    coord_flip()
```

# Calculate ranking for inverse document frequency

```{r}
unigrams_wt <- ecb_unigrams %>%
  group_by(date) %>%
  count_ngrams %>%
  bind_tf_idf(word, date, n) %>%
  ungroup

unigrams_wt %>% 
  top_n(20) %>%
  ggplot(aes(factor(word, levels = rev(unique(word))), tf_idf, fill = as.factor(year(date)))) +
    geom_col() +
    labs(x = NULL, y = "tf-idf") +
    coord_flip()
```

# Words with high weights

Below we see words with high weights due to the uniqueness of their appearance for each of the years

```{r}
unigrams_wt %>% 
  ungroup %>%
  filter(year(date) > 2008) %>%
  group_by(year = as.factor(year(date))) %>% 
  top_n(15, tf_idf) %>% 
  ungroup %>%
  ggplot(aes(factor(word, levels = rev(unique(word))), tf_idf, fill = year)) +
    geom_col(show.legend = FALSE) +
    labs(x = NULL, y = "tf-idf") +
    facet_wrap(~year, ncol = 3, scales = "free") +
    coord_flip()
```

# Calculation of weighted scores

Adjusting for weighted score, we compare with the unweighted scores. We see that directionally, both looks quite similar. 

```{r, fig.height = 5}
weighted_scores <- unigrams_wt %>%
  add_sentiments(all = TRUE) %>%
  group_by(date, method) %>%
  calc_sentiment_score(wt = "tf_idf") 

combined_scores <- bind_rows(ecb_scores %>% mutate(weight = "unweighted"),
                            weighted_scores %>% mutate(weight = "weighted"))

# Centre all scores and plot weight and unweighted on the same chart
combined_scores %>%
  group_by(method, weight) %>%
  mutate(score = scale(score)) %>%
  ggplot(aes(date, score, colour = weight)) +
  geom_line() +
  facet_wrap(~method, ncol = 1, scales = "free_y") +
  geom_hline(yintercept = 0, lwd=0.3, lty = 5) +
  ggtitle("Standardized sentiment scores")
```

# Top weighting words

We analyse the in the weighted sentiment, which are the words which are the top contributors to the sentiment scores in each of the recent years since 2008

```{r fig.height = 13, fig.width=10}
unigrams_wt %>%
  add_sentiments(all = TRUE) %>%
  group_by(date, word) %>%
  calc_sentiment_score(wt = "tf_idf") %>%
  group_by(year = year(date), word, method) %>%
  summarise(score = sum(score)) %>%
  group_by(year, method) %>%
  filter(year > 2012) %>%
  top_n(10, abs(score)) %>%
  ungroup %>%
  ggplot(aes(factor(word, levels = unique(word[order(score)])), score, fill = factor(sign(score)))) +
    geom_col(show.legend = FALSE) +
    labs(x = NULL, y = "tf-idf") +
    facet_grid(year~method, scales = "free") +
    coord_flip()
```

## TODO 3: Subtracting negation words using bigrams

## Diagnostics of using different dictionaries

### Comparison of weighted scores across the different dictionaries

Weighted scores look more or less correlated, although not very strongly

```{r}
library(GGally)
weighted_scores %>%
  select(-negative, -positive) %>%
  spread(method, score) %>%
  select(-date) %>%
  ggpairs
```


# Dealing with bigrams and negation words

```{r, fig.height=10, fig.width=8}
bigrams_separated <- df_ecb %>%
  make_ngrams(2, remove_stop_words = FALSE)

negation_words <- c("not", "no", "never", "without", "negative", "weak")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  ungroup()

##################
# adjust weighted scores
##################

# Calculate negative scores from negated words
negative_scores_by_word <- negated_words %>% 
  rename(n2 = n, word = word2) %>%
  inner_join(unigrams_wt, by = c("date", "word")) %>%
  mutate(tf_idf2 = (n2 * 2 * -1)/n * tf_idf) %>%  # We subtract twice to adjust for the "wrong" impact on the raw score
  add_sentiments(all = TRUE) %>%
  group_by(date, word, word1) %>%
  calc_sentiment_score(wt = "tf_idf2") 

# Total negative scores by date
negative_scores <- negative_scores_by_word %>%
  group_by(date, method) %>%
  summarise(score = sum(score))

# Join negative scores to original weighted scores, and recalculated combined scores
combined_scores <- negative_scores %>%
 # select(-negative, -positive) %>%
  rename(score2 = score) %>%
  right_join(weighted_scores, by = c("date", "method")) %>%
  mutate(score3 = ifelse(is.na(score2), score, score + score2))

# Plot combined weighted scores scores
combined_scores %>% select(date, method, score, score2, score3) %>%
  gather(score_type, scores, -date, -method) %>%
  ggplot(aes(date, scores, colour = score_type)) +
  geom_line() +
  facet_wrap(~method, ncol=1, scales = "free_y") +
  geom_hline(yintercept = 0, lwd=0.3, lty = 5) 

# Plot words with negated contributions
# negative_scores_by_word %>%
#   group_by(word, word1) %>%
#   summarise(score = sum(score)) %>%
#   top_n(20, desc(abs(score))) %>%
#   mutate(full_word = paste(word1, word)) %>%
#   ggplot(aes(score, full_word, fill = score > 0)) %>%
#     geom_col() %>%
#     coord_flip()
  
  

####################
# adjust unweighted scores
####################

#negated_words %>% 

# weighted_scores
# weighted_scores %>% inner_join(negative_scores, by = c("date", "method"))
# 
# negated_words %>%
#   rename(word = word2) %>%
#   add_sentiments(all = TRUE) %>%
#   group_by(date, word1, word) %>%
#   calc_sentiment_score(wt = "") %>%
#   mutate(contribution = n * score) %>%
#   group_by(word1) %>%
#   top_n(10, desc(abs(contribution))) %>%
#   arrange(contribution) %>%
#   ungroup %>%
#   mutate(word2 = reorder(word2, contribution)) %>%
#   ggplot(aes(word2, n * score, fill = n * score > 0)) +
#   geom_col(show.legend = FALSE) +
#   facet_wrap(~word1, ncol = 2, scales = "free_y") +
#   xlab("Words preceded by \"not\"") +
#   ylab("Sentiment score * number of occurrences") +
#   coord_flip() +
#   scale_color_manual(labels = c("original", "negatives", "adjusted"), values = c("red", "green", "blue"))

```

# Visualizing network of bigrams

```{r}
df_ecb %>%
  make_ngrams(2) %>%
  count_ngrams() %>%
  visualize_bigrams(min.n = 100)
```

# Counting and correlation among sections

```{r}
## POSSIBLE IDEA: Break into individual years, and find change in correlation between words for key words like inflation,
##                rate hikes, growth etc?

ecb_year_words <- ecb_unigrams %>%
  mutate(section = year(date))

library(widyr)
word_cors <- ecb_year_words %>%
  filter(section == 2016) %>%
  group_by(word) %>%
  filter(n >= 20) %>%
  pairwise_cor(word, date, sort = TRUE)
 
word_cors %>%
  filter(item1 %in% c("inflation", "growth")) %>%
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()
```

